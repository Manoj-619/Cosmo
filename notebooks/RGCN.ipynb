{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several Core processes that are used to build the RGCN model.\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Load and preprocess the knowledge graph data.\n",
    "   - Create training, validation, and test sets.\n",
    "   - Build out Node and Edge Data Loaders\n",
    "   - Build Out relationship Graph\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - Define the RGCN model structure.\n",
    "   - Specify the number of layers, hidden dimensions, and output dimensions.\n",
    "\n",
    "3. **Training**:\n",
    "   - Implement the training loop.\n",
    "   - Use appropriate loss functions and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir('/Users/cosmodenny/zavmo-api')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofqual_data = pd.read_csv('docs/nos-ofqual/ofqual_20250127-v2.csv.zip')\n",
    "nos_data = pd.read_excel('docs/nos-ofqual/latest_nos_data.xlsx')\n",
    "nos_data.to_csv('docs/nos-ofqual/latest_nos_data.csv', index=False)\n",
    "nos_data = pd.read_csv('docs/nos-ofqual/latest_nos_data.csv')\n",
    "nos_ssa_data = pd.read_csv('docs/nos-ofqual/NOS_Data_w_SSA_Industry.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column width\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "pd.set_option('display.width', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unit_key', 'owner_organisation_name', 'title', 'unit_ssa',\n",
       "       'learning_outcomes', 'ofqual_id', 'level_str', 'level', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofqual_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                                                                                Know about engineering materials, Know how to use engineering tools\n",
      "1    Work safely, Know about cooling systems, Know how to use antifreeze, Be able to pressure test a cooling system, Know about engine lubrication systems, Be aware of environmental considerations\n",
      "Name: learning_outcomes, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ofqual_learning_outcomes_eg = ofqual_data['learning_outcomes'][:2]\n",
    "print(ofqual_learning_outcomes_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                                                                                                 **500/4242/7**\\nTitle: Introduction to Engineering Equipment and Materials (Level 1)\\nSector Subject Area: 04.3 Transportation operations and maintenance\\nOrganisation: Skills and Education Group Awards\\nLearning Outcomes:\\n\\nKnow about engineering materials, Know how to use engineering tools\n",
      "1    **500/4242/7**\\nTitle: Introduction to Engine Liquid Cooling and Engine Lubrication Systems (Level 1)\\nSector Subject Area: 04.3 Transportation operations and maintenance\\nOrganisation: Skills and Education Group Awards\\nLearning Outcomes:\\n\\nWork safely, Know about cooling systems, Know how to use antifreeze, Be able to pressure test a cooling system, Know about engine lubrication systems, Be aware of environmental considerations\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ofqual_text_eg = ofqual_data['text'][:2]\n",
    "print(ofqual_text_eg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm ok so looking at this i think that we might be able to further make this data slighly better by adding in a level of granularity to the learning outcomes, e.g learning outcomes, then learning outcome 1.1, 1.2, 1.3 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nos_title', 'urn', 'suites', 'occupations', 'soc', 'developed_by',\n",
       "       'approved_on', 'web_link', 'pdf_link'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nos_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Abnormal Load Escorting\n",
      "1    Abnormal Load Escorting\n",
      "Name: suites, dtype: object\n",
      "0    https://files.ukstandards.org.uk/pdfs/SFL240TemplateF2012v2.pdf\n",
      "1      https://files.ukstandards.org.uk/pdfs/SFL241TemplateF2012.pdf\n",
      "Name: pdf_link, dtype: object\n"
     ]
    }
   ],
   "source": [
    "nos_suites_eg = nos_data['suites'][:2]\n",
    "print(nos_suites_eg)\n",
    "nos_pdf_eg = nos_data['pdf_link'][:2]\n",
    "print(nos_pdf_eg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so the links then provide the following information:\n",
    "Performance Criteria\n",
    "P1-Pn\n",
    "Knowledge and Understanding\n",
    "K1-Kn\n",
    "\n",
    "Essentially in my head we are trying to create a tree slide of information right.\n",
    "\n",
    "You Have:\n",
    "SSA -> Sub_SSA -> Industry -> NOS -> Competency -> Knowledge and Understanding -> Performance Criteria\n",
    "                               |        |\n",
    "                             OFQUAL -> Unit -> Learning Outcome -> Knowledge and Understanding -> Performance Criteria\n",
    "\n",
    "I'm going to get an example OFQUAL from our Agent to find one that we can use to build an example tree slide of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nos_id', 'NOS_Industry', 'title', 'text', 'OFQUAL_SSA',\n",
       "       'Additional OFQUAL_SSA', 'Additional_OFQUAL_SSA.2',\n",
       "       'Additional_OFQUAL_SSA.3', 'Additional_OFQUAL_SSA.4',\n",
       "       'Additional_OFQUAL_SSA.5', 'Additional_OFQUAL_SSA.6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nos_ssa_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok in nos_data we have nos_id, Nos_text, nos_industry, ofqual_ssa (this is a sector subject area(ssa), that at a surface level\n",
    "maps our ofqual and nos together, there are 50 total SSAs and a couple hundred nos industries, ideally we want to create a tree of parent child grandchil subnode for industry then also have some sort of mapping based off competency and ofqual unit, could be some sort of similarity and then overtime based off maping score we can build relationships) \n",
    "\n",
    "so essentially we can map node_types based off OFQUAL, NOS, SSA, Sub_SSA, Industry, Competency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_types = {\n",
    "    'NOS': ['nos_id', 'nos_text'],  # NOS units\n",
    "    'OFQUAL': ['ofqual_id', 'ofqual_text'],  # OFQUAL units\n",
    "    'SSA': ['ssa_code', 'ssa_description'],  # Sector Subject Area Integar e.g 5 Construction, Planning and the Built Environment\n",
    "    'Sub_SSA': ['sub_ssa_code', 'sub_ssa_description'],  # Sub Sector Subject Areas, e.g 5.1 - Architecture\n",
    "    'Industry': ['industry_id', 'industry_name'],  # Industry categories\n",
    "    'Competency': ['comp_id', 'comp_description']  # Competencies extracted from text\n",
    "}\n",
    "\n",
    "edge_types = {\n",
    "    ('NOS', 'belongs_to', 'SSA'),\n",
    "    ('OFQUAL', 'belongs_to', 'Sub_SSA'),\n",
    "    ('NOS', 'part_of', 'Industry'),\n",
    "    ('Industry', 'belongs_to', 'Sub_SSA'),\n",
    "    ('NOS', 'requires', 'Competency'),\n",
    "    ('OFQUAL', 'requires', 'Competency'),\n",
    "    ('Industry', 'parent_of', 'Industry'),  # Hierarchical industry relationships With linking into SSA and Sub_SSA\n",
    "    ('NOS', 'similar_to', 'OFQUAL'),  # Similarity-based mapping\n",
    "    ('Competency', 'related_to', 'Competency')  # Competency relationships\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went with PyG for this because I know it best then also GDL is now depreciated lol, but if you have used torch before this is veery similar but optimised for graph data, the RGCN works by having both node types (NOS, OFQUAL, SSA etc.) and edge types (belongs_to, requires, similar_to etc.) this gives us the ability to essentially assign a definition and then see how it reacts, \n",
    "\n",
    "look at https://www.youtube.com/watch?v=OI0Jo-5d190&list=PLSgGvve8UweGx4_6hhrF3n4wpHf_RV76_ (This was my intro to GNNs)\n",
    "\n",
    "\n",
    "The RGCN model can handle:\n",
    "1. Multiple node types (NOS, OFQUAL, SSA etc.)\n",
    "2. Multiple edge types (belongs_to, requires, similar_to etc.) \n",
    "3. Message passing between different types of nodes\n",
    "\n",
    "This makes it ideal for:\n",
    "- Analysing relationships between entities (like Social Network Analysis)\n",
    "- Predicting properties of nodes based on their neighborhood structure and connections e.g our relation to nos and ofqual competencies\n",
    "- Learning embeddings that capture the semantic relationships in our heterogeneous graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cosmodenny/zavmo-api/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, num_rels, num_bases=-1): \n",
    "        super(RGCN, self).__init__()\n",
    "        # num_bases is the number of bases to use for the RGCN layers, if -1 is used then it will use all the bases\n",
    "        # in_dim is the dimension of the input features\n",
    "        # h_dim is the dimension of the hidden features\n",
    "        # out_dim is the dimension of the output features\n",
    "        # num_rels is the number of relations in the graph\n",
    "        \n",
    "\n",
    "        # Input layer -> this is where we define our node types and then embed it.\n",
    "        self.embedding = nn.ModuleDict({\n",
    "            'NOS': nn.Linear(in_dim['NOS'], h_dim),\n",
    "            'OFQUAL': nn.Linear(in_dim['OFQUAL'], h_dim),\n",
    "            'SSA': nn.Linear(in_dim['SSA'], h_dim),\n",
    "            'Industry': nn.Linear(in_dim['Industry'], h_dim),\n",
    "            'Competency': nn.Linear(in_dim['Competency'], h_dim)\n",
    "        })\n",
    "        \n",
    "        # RGCN layers using PyG -> this is where we define the number of layers (convolutions specifically)\n",
    "        self.layers = nn.ModuleList([\n",
    "            RGCNConv(h_dim, h_dim, num_relations=num_rels, num_bases=num_bases)\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "        \n",
    "        # Transformer layer for attention\n",
    "        self.attention = TransformerConv(h_dim, h_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output = nn.Linear(h_dim, out_dim)  \n",
    "        # Our output dimension for our embeddings, normally mismatch here causes a good amount of bug fixing\n",
    "        \n",
    "        # Similarity scorer using cosine similarity in the output dimension to allow us to better track the output similarity\n",
    "        self.similarity = nn.CosineSimilarity(dim=out_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        # Embed different node types\n",
    "        h = {\n",
    "            ntype: self.embedding[ntype](feat)\n",
    "            for ntype, feat in x.items()\n",
    "        }\n",
    "        \n",
    "        # Combine embeddings -> This is because due to the complexity of the graph there will likely be multiple nodes of different shapes,\n",
    "        # so we need to combine them into a single tensor to maintain order in our dictionary essentially this is our preprocessing\n",
    "        # because our RGCN expects a single tensor.\n",
    "        h_combined = torch.cat([h[ntype] for ntype in h.keys()], dim=0)\n",
    "        \n",
    "        # Apply RGCN layers\n",
    "        # This is where we apply the RGCN layers to our combined embeddings, this is where the magic happens!\n",
    "        for layer in self.layers:\n",
    "            h_combined = layer(h_combined, edge_index, edge_type)\n",
    "            h_combined = torch.relu(h_combined)\n",
    "        \n",
    "        # Apply attention\n",
    "        # This is essentially where we look at the relationship between the nodes, it does this by taking each node and having a look at its neighbours,\n",
    "        # Then it will calulate the attention scores (how important is each neightbour), and normalise this, the transformer conv uses softmax i think, \n",
    "        # so it just takes the whole thing and turns it into a probability distribution over the neighbours, then this gives us the normalised attention scores\n",
    "        # that tell us the relationship between 0 and 1 e.g 0.8 strong relationship, 0.3 weak relationship.\n",
    "        h_combined = self.attention(h_combined, edge_index)\n",
    "        \n",
    "        # Output projection\n",
    "        # Then we take our attention scores and pass them through out output layer to get the final embeddings, which we can then decode. simples\n",
    "        out = self.output(h_combined)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def score_similarity(self, h1, h2):\n",
    "        return self.similarity(h1, h2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
