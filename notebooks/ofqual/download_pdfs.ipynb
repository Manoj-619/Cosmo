{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDFs. Downloading...\n",
      "Downloaded: C:\\Users\\amith\\Kenpath\\OFQUAL\\othm_pdfs\\OTHM-Customer-Service-Statement-2023-09-11.pdf\n",
      "Downloaded: C:\\Users\\amith\\Kenpath\\OFQUAL\\othm_pdfs\\othm-guidance-statement-to-centres-on-the-risk-of-ai.pdf\n",
      "Downloaded: C:\\Users\\amith\\Kenpath\\OFQUAL\\othm_pdfs\\othm_level_7_diploma_in_data_science_specification_february_2023_2023-07-26_10-33.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL to scrape PDFs from\n",
    "BASE_URL = \"https://othm.org.uk/qualification/othm-level-7-diploma-in-data-science\"\n",
    "# Directory to save downloaded PDFs\n",
    "DOWNLOAD_FOLDER = r\"C:\\Users\\amith\\Kenpath\\OFQUAL\\othm_pdfs\"\n",
    "\n",
    "def get_pdf_links(url):\n",
    "    \"\"\"Fetch all PDF links from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags with href containing .pdf\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"]\n",
    "        if href.endswith(\".pdf\"):\n",
    "            full_url = urljoin(url, href)  # Handle relative URLs\n",
    "            pdf_links.append(full_url)\n",
    "\n",
    "    return pdf_links\n",
    "\n",
    "def download_pdf(url, folder):\n",
    "    \"\"\"Download a PDF file and save it locally.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        filename = os.path.join(folder, os.path.basename(url))\n",
    "        with open(filename, \"wb\") as pdf_file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                pdf_file.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {url}\")\n",
    "\n",
    "\n",
    "os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "pdf_links = get_pdf_links(BASE_URL)\n",
    "if not pdf_links:\n",
    "    print(\"No PDF files found.\")\n",
    "\n",
    "print(f\"Found {len(pdf_links)} PDFs. Downloading...\")\n",
    "for pdf_url in pdf_links:\n",
    "    download_pdf(pdf_url, DOWNLOAD_FOLDER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the webpage containing PDF links\n",
    "page_url = \"https://www.highfieldqualifications.com/products/qualifications/quality-assurance/external-level-4\"\n",
    "\n",
    "# Folder to save PDFs\n",
    "download_folder = r\"C:\\Users\\amith\\Kenpath\\OFQUAL\\highfield_pdfs\"\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# Fetch the webpage\n",
    "response = requests.get(page_url)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parse the webpage\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all <a> tags\n",
    "pdf_links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "# Process and download each link\n",
    "count = 515 # Counter for naming files\n",
    "for link in pdf_links:\n",
    "    full_url = urljoin(page_url, link)  # Convert relative URLs to absolute\n",
    "\n",
    "    # Check if the URL is a PDF by making a HEAD request\n",
    "    head_response = requests.head(full_url, allow_redirects=True)\n",
    "    content_type = head_response.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "    if \"pdf\" in content_type.lower():  # Confirm it's a PDF\n",
    "        pdf_name = os.path.join(download_folder, f\"ofqual{count}.pdf\")  # Sequential naming\n",
    "\n",
    "        # Download the PDF\n",
    "        pdf_response = requests.get(full_url, stream=True)\n",
    "        if pdf_response.status_code == 200:\n",
    "            with open(pdf_name, \"wb\") as file:\n",
    "                for chunk in pdf_response.iter_content(chunk_size=1024):\n",
    "                    file.write(chunk)\n",
    "            print(f\"Downloaded: {pdf_name}\")\n",
    "            count += 1  # Increment the counter\n",
    "        else:\n",
    "            print(f\"Failed to download {full_url}. Status code: {pdf_response.status_code}\")\n",
    "\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zavevn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
