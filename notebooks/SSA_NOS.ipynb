{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nos_id', 'NOS_Industry', 'title', 'text', 'OFQUAL_SSA',\n",
       "       'Additional OFQUAL_SSA', 'Additional_OFQUAL_SSA.2',\n",
       "       'Additional_OFQUAL_SSA.3', 'Additional_OFQUAL_SSA.4',\n",
       "       'Additional_OFQUAL_SSA.5', 'Additional_OFQUAL_SSA.6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nos_ssa = pd.read_csv(\"../docs/nos-ofqual/NOS_Data_w_SSA_Industry.csv\")\n",
    "nos_ssa.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14.1 Foundations for learning and life'\n",
      " '10.2 Archaeology and archaeological sciences'\n",
      " '7.1 Retailing and wholesaling' '1.4 Public services' nan\n",
      " '5.2 Building and construction' '15.3 Business management'\n",
      " '11.2 Sociology and social policy' '15.1 Accounting and finance'\n",
      " '4.1 Engineering' '3.1 Environmental conservation'\n",
      " '4.2 Manufacturing technologies' '9.2 Crafts, creative arts and design'\n",
      " '9.1 Performing arts' '3.4 Environmental conservation'\n",
      " '7.2 Warehousing and distribution' '7.4 Hospitality and catering'\n",
      " '3.3 Animal care and veterinary science' '3.2 Horticulture and forestry'\n",
      " '12.2 Other languages literature and culture' '15.4 Marketing and Sales'\n",
      " '1.3 Health and social care' '9.3 Media and communication'\n",
      " '13.1 Teaching and Lecturing' '2.1 Science'\n",
      " '4.3 Transportation operations and maintenance'\n",
      " '5.3 Urban, rural and regional planning'\n",
      " '6.1 Digital technology (practitioners)'\n",
      " '18.1 Publishing and information services'\n",
      " '8.1 Sport leisure and recreation' '3.1 Agriculture'\n",
      " '15.3 Business Management' '1.5 Child development and well-being'\n",
      " '13.2 Direct learning support'\n",
      " '1.2 Nursing, and subjects and vocations allied to medicine'\n",
      " '15.5 Law and legal services' '7.3 Service enterprises' '10.1 History']\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "SSA_unique_nos = nos_ssa['OFQUAL_SSA'].unique()\n",
    "print(SSA_unique_nos)\n",
    "print(len(SSA_unique_nos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9205\n",
      "16052\n"
     ]
    }
   ],
   "source": [
    "len_nos = len(nos_ssa)\n",
    "ssa_nos_count = len(nos_ssa['OFQUAL_SSA'].dropna())\n",
    "print(ssa_nos_count)\n",
    "print(len_nos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.34487914278594 % of NOS have SSA\n"
     ]
    }
   ],
   "source": [
    "percentage_nos = ((ssa_nos_count/len_nos) * 100) \n",
    "print(percentage_nos,\"% of NOS have SSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         nos_id                     NOS_Industry  \\\n",
      "0        CFAUE3         Understanding Enterprise   \n",
      "1      CCSAPAA2          Archaeological Practice   \n",
      "2        CFAUE6         Understanding Enterprise   \n",
      "3       IMIHR15                   Vehicle Rental   \n",
      "4        CFAUE7         Understanding Enterprise   \n",
      "...         ...                              ...   \n",
      "16044     SFHM3  Breast Screening and Assessment   \n",
      "16045     SFHM9  Breast Screening and Assessment   \n",
      "16048      REC2                      Recruitment   \n",
      "16049    CFAUE5         Understanding Enterprise   \n",
      "16050      REC1                      Recruitment   \n",
      "\n",
      "                                                   title  \\\n",
      "0            Knowing the market and satisfying customers   \n",
      "1                                    Commission Research   \n",
      "2                                   Planning for Success   \n",
      "3      Receive goods, equipment or vehicles for hire ...   \n",
      "4                                 Managing Money Matters   \n",
      "...                                                  ...   \n",
      "16044  Review mammography images for the presence of ...   \n",
      "16045     Undertake a clinical examination of the breast   \n",
      "16048  Contact hirers and establish recruitment requi...   \n",
      "16049                           Winning Help and Support   \n",
      "16050                      Develop a resourcing strategy   \n",
      "\n",
      "                                                    text  \\\n",
      "0      **NOS ID:** CFAUE3  \\n**Title:** Knowing the m...   \n",
      "1      **NOS ID:** CCSAPAA2  \\n**Title:** Commission ...   \n",
      "2      **NOS ID:** CFAUE6  \\n**Title:** Planning for ...   \n",
      "3      **NOS ID:** IMIHR15  \\n**Title:** Receive good...   \n",
      "4      **NOS ID:** CFAUE7  \\n**Title:** Managing Mone...   \n",
      "...                                                  ...   \n",
      "16044  **NOS ID:** sfhm3  \\n**Title:** Review mammogr...   \n",
      "16045  **NOS ID:** sfhm9  \\n**Title:** Undertake a cl...   \n",
      "16048  **NOS ID:** REC2  \\n**Title:** Contact hirers ...   \n",
      "16049  **NOS ID:** CFAUE5  \\n**Title:** Winning Help ...   \n",
      "16050  **NOS ID:** REC1  \\n**Title:** Develop a resou...   \n",
      "\n",
      "                                              OFQUAL_SSA  \n",
      "0                 14.1 Foundations for learning and life  \n",
      "1           10.2 Archaeology and archaeological sciences  \n",
      "2                 14.1 Foundations for learning and life  \n",
      "3                          7.1 Retailing and wholesaling  \n",
      "4                 14.1 Foundations for learning and life  \n",
      "...                                                  ...  \n",
      "16044  1.2 Nursing, and subjects and vocations allied...  \n",
      "16045  1.2 Nursing, and subjects and vocations allied...  \n",
      "16048                           15.3 Business management  \n",
      "16049             14.1 Foundations for learning and life  \n",
      "16050                           15.3 Business management  \n",
      "\n",
      "[9196 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "nos_ssa_list = nos_ssa[['nos_id', 'NOS_Industry', 'title', 'text', 'OFQUAL_SSA']].dropna()\n",
    "print(nos_ssa_list)\n",
    "nos_ssa_list.to_csv('nos_ssa_list_57%.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. Data Preparation\n",
    "def prepare_text_features(nos_data):\n",
    "    # Fill NaN values with empty strings\n",
    "    title = nos_data['title'].fillna('')\n",
    "    text = nos_data['text'].fillna('')\n",
    "    industry = nos_data['NOS_Industry'].fillna('')\n",
    "    \n",
    "    # Combine relevant text fields\n",
    "    return title + ' ' + text + ' ' + industry\n",
    "\n",
    "# 2. Create the classification pipeline\n",
    "def create_classifier():\n",
    "    return Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english'\n",
    "        )),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced'\n",
    "        )))\n",
    "    ])\n",
    "\n",
    "# 3. Training and Evaluation\n",
    "def train_and_evaluate(X, y):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    classifier = create_classifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = classifier.score(X_train, y_train)\n",
    "    test_score = classifier.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Training accuracy: {train_score:.3f}\")\n",
    "    print(f\"Testing accuracy: {test_score:.3f}\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "# 4. Prediction with confidence scores\n",
    "def predict_with_confidence(classifier, text):\n",
    "    # Handle NaN values in prediction data\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = text.fillna('')\n",
    "    \n",
    "    # Get probability scores\n",
    "    proba = classifier.predict_proba(text)\n",
    "    predictions = classifier.predict(text)\n",
    "    \n",
    "    # Get confidence scores for each prediction\n",
    "    confidence_scores = np.max(proba, axis=1)\n",
    "    \n",
    "    return predictions, confidence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.850\n",
      "Testing accuracy: 0.785\n",
      "\n",
      "Sample predictions:\n",
      "     nos_id              NOS_Industry  \\\n",
      "0    CFAUE3  Understanding Enterprise   \n",
      "1  CCSAPAA2   Archaeological Practice   \n",
      "2    CFAUE6  Understanding Enterprise   \n",
      "3   IMIHR15            Vehicle Rental   \n",
      "4    CFAUE7  Understanding Enterprise   \n",
      "\n",
      "                                     OFQUAL_SSA  \\\n",
      "0        14.1 Foundations for learning and life   \n",
      "1  10.2 Archaeology and archaeological sciences   \n",
      "2        14.1 Foundations for learning and life   \n",
      "3                 7.1 Retailing and wholesaling   \n",
      "4        14.1 Foundations for learning and life   \n",
      "\n",
      "                                  predicted_SSA  confidence_score  \n",
      "0        14.1 Foundations for learning and life          0.225326  \n",
      "1  10.2 Archaeology and archaeological sciences          0.364586  \n",
      "2        14.1 Foundations for learning and life          0.356810  \n",
      "3                 7.1 Retailing and wholesaling          0.301787  \n",
      "4        14.1 Foundations for learning and life          0.329046  \n",
      "\n",
      "Predictions saved to nos_ssa_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare training data\n",
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "nos_ssa_list\n",
    "\n",
    "# Prepare features and labels\n",
    "X = prepare_text_features(nos_ssa_list)\n",
    "y = nos_ssa_list['OFQUAL_SSA']\n",
    "\n",
    "# Train the model\n",
    "classifier = train_and_evaluate(X, y)\n",
    "\n",
    "# Load prediction data\n",
    "pred_data = pd.read_csv('../docs/nos-ofqual/NOS_Data_w_SSA_Industry.csv')\n",
    "\n",
    "# Prepare prediction features\n",
    "X_pred = prepare_text_features(pred_data)\n",
    "\n",
    "# Make predictions with confidence scores\n",
    "predictions, confidence_scores = predict_with_confidence(classifier, X_pred)\n",
    "\n",
    "# Add predictions and confidence scores to prediction dataframe\n",
    "pred_data['predicted_SSA'] = predictions\n",
    "pred_data['confidence_score'] = confidence_scores\n",
    "\n",
    "# Display sample of predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "print(pred_data[['nos_id', 'NOS_Industry', 'OFQUAL_SSA', 'predicted_SSA', 'confidence_score']].head())\n",
    "\n",
    "# Save predictions\n",
    "pred_data.to_csv('nos_ssa_predictions.csv', index=False)\n",
    "print(\"\\nPredictions saved to nos_ssa_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "Training accuracy: 0.863\n",
      "Testing accuracy: 0.772\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.770\n",
      "\n",
      "Epoch 2:\n",
      "Training accuracy: 0.863\n",
      "Testing accuracy: 0.772\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.770\n",
      "\n",
      "Epoch 3:\n",
      "Training accuracy: 0.863\n",
      "Testing accuracy: 0.772\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.771\n",
      "\n",
      "Epoch 4:\n",
      "Training accuracy: 0.863\n",
      "Testing accuracy: 0.772\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.771\n",
      "\n",
      "Epoch 5:\n",
      "Training accuracy: 0.863\n",
      "Testing accuracy: 0.771\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.770\n",
      "\n",
      "Epoch 6:\n",
      "Training accuracy: 0.864\n",
      "Testing accuracy: 0.771\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.769\n",
      "\n",
      "Epoch 7:\n",
      "Training accuracy: 0.864\n",
      "Testing accuracy: 0.771\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.769\n",
      "\n",
      "Epoch 8:\n",
      "Training accuracy: 0.864\n",
      "Testing accuracy: 0.770\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.768\n",
      "\n",
      "Epoch 9:\n",
      "Training accuracy: 0.864\n",
      "Testing accuracy: 0.769\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.768\n",
      "\n",
      "Epoch 10:\n",
      "Training accuracy: 0.864\n",
      "Testing accuracy: 0.768\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.767\n",
      "\n",
      "Epoch 11:\n",
      "Training accuracy: 0.864\n",
      "Testing accuracy: 0.767\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.766\n",
      "\n",
      "Epoch 12:\n",
      "Training accuracy: 0.864\n",
      "Testing accuracy: 0.767\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.766\n",
      "\n",
      "Epoch 13:\n",
      "Training accuracy: 0.864\n",
      "Testing accuracy: 0.767\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.766\n",
      "\n",
      "Epoch 14:\n",
      "Training accuracy: 0.864\n",
      "Testing accuracy: 0.767\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.766\n",
      "\n",
      "Epoch 15:\n",
      "Training accuracy: 0.865\n",
      "Testing accuracy: 0.767\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.766\n",
      "\n",
      "Epoch 16:\n",
      "Training accuracy: 0.865\n",
      "Testing accuracy: 0.767\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.766\n",
      "\n",
      "Epoch 17:\n",
      "Training accuracy: 0.865\n",
      "Testing accuracy: 0.767\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.766\n",
      "\n",
      "Epoch 18:\n",
      "Training accuracy: 0.865\n",
      "Testing accuracy: 0.767\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.766\n",
      "\n",
      "Epoch 19:\n",
      "Training accuracy: 0.865\n",
      "Testing accuracy: 0.767\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.766\n",
      "\n",
      "Epoch 20:\n",
      "Training accuracy: 0.865\n",
      "Testing accuracy: 0.767\n",
      "Cross-validation mean: 0.777 (+/- 0.018)\n",
      "F-1 Score: 0.766\n",
      "\n",
      "Training Summary:\n",
      "Epoch 1: Train=0.863, Test=0.772, CV=0.777±0.018\n",
      "Epoch 2: Train=0.863, Test=0.772, CV=0.777±0.018\n",
      "Epoch 3: Train=0.863, Test=0.772, CV=0.777±0.018\n",
      "Epoch 4: Train=0.863, Test=0.772, CV=0.777±0.018\n",
      "Epoch 5: Train=0.863, Test=0.771, CV=0.777±0.018\n",
      "Epoch 6: Train=0.864, Test=0.771, CV=0.777±0.018\n",
      "Epoch 7: Train=0.864, Test=0.771, CV=0.777±0.018\n",
      "Epoch 8: Train=0.864, Test=0.770, CV=0.777±0.018\n",
      "Epoch 9: Train=0.864, Test=0.769, CV=0.777±0.018\n",
      "Epoch 10: Train=0.864, Test=0.768, CV=0.777±0.018\n",
      "Epoch 11: Train=0.864, Test=0.767, CV=0.777±0.018\n",
      "Epoch 12: Train=0.864, Test=0.767, CV=0.777±0.018\n",
      "Epoch 13: Train=0.864, Test=0.767, CV=0.777±0.018\n",
      "Epoch 14: Train=0.864, Test=0.767, CV=0.777±0.018\n",
      "Epoch 15: Train=0.865, Test=0.767, CV=0.777±0.018\n",
      "Epoch 16: Train=0.865, Test=0.767, CV=0.777±0.018\n",
      "Epoch 17: Train=0.865, Test=0.767, CV=0.777±0.018\n",
      "Epoch 18: Train=0.865, Test=0.767, CV=0.777±0.018\n",
      "Epoch 19: Train=0.865, Test=0.767, CV=0.777±0.018\n",
      "Epoch 20: Train=0.865, Test=0.767, CV=0.777±0.018\n",
      "\n",
      "Sample predictions with best model:\n",
      "     nos_id              NOS_Industry  \\\n",
      "0    CFAUE3  Understanding Enterprise   \n",
      "1  CCSAPAA2   Archaeological Practice   \n",
      "2    CFAUE6  Understanding Enterprise   \n",
      "3   IMIHR15            Vehicle Rental   \n",
      "4    CFAUE7  Understanding Enterprise   \n",
      "\n",
      "                                     OFQUAL_SSA  \\\n",
      "0        14.1 Foundations for learning and life   \n",
      "1  10.2 Archaeology and archaeological sciences   \n",
      "2        14.1 Foundations for learning and life   \n",
      "3                 7.1 Retailing and wholesaling   \n",
      "4        14.1 Foundations for learning and life   \n",
      "\n",
      "                                  predicted_SSA  confidence_score  \n",
      "0        14.1 Foundations for learning and life          0.479106  \n",
      "1  10.2 Archaeology and archaeological sciences          0.833026  \n",
      "2        14.1 Foundations for learning and life          0.517178  \n",
      "3                 7.1 Retailing and wholesaling          0.465127  \n",
      "4        14.1 Foundations for learning and life          0.519820  \n",
      "\n",
      "Predictions saved to nos_ssa_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Create classifier with SGD\n",
    "# Modified feature engineering\n",
    "classifier = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=150000,  # 3x increase from 50k\n",
    "        ngram_range=(1, 4),   # Carefully expanded\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True,\n",
    "        min_df=2,            # Slightly more permissive\n",
    "        max_df=0.6,          # Slightly more restrictive\n",
    "        analyzer='word',     # New - explicit analyzer\n",
    "        norm='l2'            # New - better normalization\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(SGDClassifier(\n",
    "        loss='log_loss',\n",
    "        penalty='elasticnet',\n",
    "        alpha=1e-5,          # Reduced regularization\n",
    "        learning_rate='adaptive',\n",
    "        eta0=0.2,           # Higher initial rate\n",
    "        power_t=0.25,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.25,  # More validation data\n",
    "        n_iter_no_change=7, # More patience\n",
    "        random_state=42,\n",
    "        class_weight='balanced'  # New - handle class imbalance\n",
    "    )))\n",
    "])\n",
    "\n",
    "\n",
    "def train_and_evaluate_multiple_epochs(classifier, X, y, n_epochs=5, n_splits=5):\n",
    "    # Initialize KFold cross-validator\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Track metrics across epochs\n",
    "    epoch_metrics = []\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    # Split into train/test once to have consistent test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Initial fit to transform the data\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Adjust learning rate dynamically\n",
    "        new_eta = 0.1 / (1 + 0.2 * epoch)  # Decaying learning rate\n",
    "        for estimator in classifier.named_steps['clf'].estimators_:\n",
    "            estimator.eta0 = new_eta\n",
    "            \n",
    "        # Partial fit for incremental learning\n",
    "        classifier.named_steps['clf'].partial_fit(\n",
    "            classifier.named_steps['tfidf'].transform(X_train),\n",
    "            y_train,\n",
    "            classes=np.unique(y)\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        train_score = classifier.score(X_train, y_train)\n",
    "        test_score = classifier.score(X_test, y_test)\n",
    "        \n",
    "        # Get predictions for F1 score calculation\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        \n",
    "        # Cross validation score\n",
    "        cv_scores = cross_val_score(classifier, X_train, y_train, cv=kf)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}:\")\n",
    "        print(f\"Training accuracy: {train_score:.3f}\")\n",
    "        print(f\"Testing accuracy: {test_score:.3f}\")\n",
    "        print(f\"Cross-validation mean: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        print(f\"F-1 Score: {f1_score(y_test, test_predictions, average='weighted'):.3f}\")  # Modified this line\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_metrics.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_score': train_score,\n",
    "            'test_score': test_score,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std()\n",
    "        })\n",
    "        \n",
    "        # Update best state if improved\n",
    "        if test_score > best_accuracy:\n",
    "            best_accuracy = test_score\n",
    "            best_state = {\n",
    "                'tfidf': classifier.named_steps['tfidf'],\n",
    "                'clf': classifier.named_steps['clf']\n",
    "            }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nTraining Summary:\")\n",
    "    for metric in epoch_metrics:\n",
    "        print(f\"Epoch {metric['epoch']}: \"\n",
    "              f\"Train={metric['train_score']:.3f}, \"\n",
    "              f\"Test={metric['test_score']:.3f}, \"\n",
    "              f\"CV={metric['cv_mean']:.3f}±{metric['cv_std']*2:.3f}\")\n",
    "    \n",
    "    # Restore best state\n",
    "    classifier.named_steps['tfidf'] = best_state['tfidf']\n",
    "    classifier.named_steps['clf'] = best_state['clf']\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "# Train the classifier\n",
    "classifier = train_and_evaluate_multiple_epochs(classifier, X, y, n_epochs=20)\n",
    "\n",
    "# Make predictions\n",
    "X_pred = prepare_text_features(pred_data)\n",
    "predictions, confidence_scores = predict_with_confidence(classifier, X_pred)\n",
    "\n",
    "# Add predictions and confidence scores\n",
    "pred_data['predicted_SSA'] = predictions\n",
    "pred_data['confidence_score'] = confidence_scores\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\nSample predictions with best model:\")\n",
    "sample_df = pred_data[['nos_id', 'NOS_Industry', 'OFQUAL_SSA', 'predicted_SSA', 'confidence_score']].head()\n",
    "print(sample_df)\n",
    "\n",
    "# Save predictions\n",
    "pred_data.to_csv('nos_ssa_predictions.csv', index=False)\n",
    "print(\"\\nPredictions saved to nos_ssa_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS memory allocated: 5.22 GB\n",
      "MPS max memory allocated: 10.67 GB\n",
      "MPS memory allocated: 0.04 GB\n",
      "MPS max memory allocated: 10.67 GB\n"
     ]
    }
   ],
   "source": [
    "# Check MPS memory usage\n",
    "print(f\"MPS memory allocated: {torch.mps.current_allocated_memory() / 1024**3:.2f} GB\")\n",
    "print(f\"MPS max memory allocated: {torch.mps.recommended_max_memory() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Clear GPU memory\n",
    "import torch\n",
    "torch.mps.empty_cache()  # Clear MPS (Apple GPU) memory\n",
    "\n",
    "# Also clear CUDA memory if you were using it\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Force Python garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Check MPS memory usage\n",
    "print(f\"MPS memory allocated: {torch.mps.current_allocated_memory() / 1024**3:.2f} GB\")\n",
    "print(f\"MPS max memory allocated: {torch.mps.recommended_max_memory() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1: Loss=3.5291, Acc=0.7218\n",
      "Epoch 2: Loss=3.3499, Acc=0.0291\n",
      "Epoch 3: Loss=3.2126, Acc=0.7715\n",
      "Epoch 4: Loss=3.1122, Acc=0.7897\n",
      "Epoch 5: Loss=3.0377, Acc=0.8035\n",
      "Epoch 6: Loss=2.9807, Acc=0.0285\n",
      "Epoch 7: Loss=2.9360, Acc=0.0282\n",
      "Epoch 8: Loss=2.9003, Acc=0.8276\n",
      "Epoch 9: Loss=2.8712, Acc=0.8334\n",
      "Epoch 10: Loss=2.8471, Acc=0.0274\n",
      "Epoch 11: Loss=2.8271, Acc=0.0275\n",
      "Epoch 12: Loss=2.8102, Acc=0.0281\n",
      "Epoch 13: Loss=2.7957, Acc=0.0279\n",
      "Epoch 14: Loss=2.7834, Acc=0.0275\n",
      "Epoch 15: Loss=2.7726, Acc=0.0268\n",
      "Epoch 16: Loss=2.7633, Acc=0.8516\n",
      "Epoch 17: Loss=2.7551, Acc=0.8518\n",
      "Epoch 18: Loss=2.7479, Acc=0.8527\n",
      "Epoch 19: Loss=2.7416, Acc=0.8530\n",
      "Epoch 20: Loss=2.7358, Acc=0.8532\n",
      "Epoch 21: Loss=2.7308, Acc=0.8535\n",
      "Epoch 22: Loss=2.7263, Acc=0.8537\n",
      "Epoch 1: Loss=3.5294, Acc=0.7216\n",
      "Epoch 2: Loss=3.3508, Acc=0.7511\n",
      "Epoch 3: Loss=3.2135, Acc=0.7736\n",
      "Epoch 4: Loss=3.1129, Acc=0.7909\n",
      "Epoch 5: Loss=3.0382, Acc=0.8038\n",
      "Epoch 6: Loss=2.9812, Acc=0.8122\n",
      "Epoch 7: Loss=2.9365, Acc=0.8206\n",
      "Epoch 8: Loss=2.9007, Acc=0.8272\n",
      "Epoch 9: Loss=2.8715, Acc=0.8323\n",
      "Epoch 10: Loss=2.8475, Acc=0.8372\n",
      "Epoch 11: Loss=2.8273, Acc=0.8412\n",
      "Epoch 12: Loss=2.8104, Acc=0.8441\n",
      "Epoch 13: Loss=2.7959, Acc=0.8461\n",
      "Epoch 14: Loss=2.7836, Acc=0.8488\n",
      "Epoch 15: Loss=2.7728, Acc=0.8502\n",
      "Epoch 16: Loss=2.7635, Acc=0.8512\n",
      "Epoch 17: Loss=2.7553, Acc=0.8520\n",
      "Epoch 18: Loss=2.7481, Acc=0.8527\n",
      "Epoch 19: Loss=2.7417, Acc=0.8531\n",
      "Epoch 20: Loss=2.7360, Acc=0.8534\n",
      "Epoch 21: Loss=2.7309, Acc=0.8536\n",
      "Epoch 22: Loss=2.7264, Acc=0.8537\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 8.97 GB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m         _, predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m le\u001b[38;5;241m.\u001b[39minverse_transform(predictions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m--> 112\u001b[0m pred_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_SSA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpytorch_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 107\u001b[0m, in \u001b[0;36mpytorch_predict\u001b[0;34m(model, le, text_data)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpytorch_predict\u001b[39m(model, le, text_data):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 107\u001b[0m         X_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_tensor_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(X_tensor)\n\u001b[1;32m    109\u001b[0m         _, predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 91\u001b[0m, in \u001b[0;36mprepare_tensor_features\u001b[0;34m(text_data)\u001b[0m\n\u001b[1;32m     88\u001b[0m X_tfidf \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(text_data)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch tensors\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid buffer size: 8.97 GB"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check for MPS availability\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Create PyTorch model\n",
    "class SSAClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_size, num_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# 3. Modified training function\n",
    "def train_pytorch_model(classifier, X, y, n_epochs=50):\n",
    "    # Convert labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_tensor = torch.tensor(y_encoded, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get tensor features\n",
    "    X_tensor = prepare_tensor_features(X)\n",
    "    \n",
    "    # Create model\n",
    "    input_size = X_tensor.shape[1]\n",
    "    num_classes = len(le.classes_)\n",
    "    model = SSAClassifier(input_size, num_classes).to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True,)\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_x, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            acc = (predicted == y_tensor).float().mean()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss/len(loader):.4f}, Acc={acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    return model, le\n",
    "\n",
    "\n",
    "# Modified data preparation functions\n",
    "def prepare_text_features(nos_data):\n",
    "    \"\"\"Handle both DataFrames and preprocessed Series\"\"\"\n",
    "    if isinstance(nos_data, pd.DataFrame):\n",
    "        # For raw DataFrames\n",
    "        return nos_data['title'].fillna('') + ' ' + \\\n",
    "            nos_data['text'].fillna('') + ' ' + \\\n",
    "            nos_data['NOS_Industry'].fillna('')\n",
    "    # For already processed text\n",
    "    return nos_data.fillna('')\n",
    "\n",
    "def prepare_tensor_features(text_data):\n",
    "    \"\"\"Handle preprocessed text directly\"\"\"\n",
    "    # Vectorize using existing TF-IDF\n",
    "    X_tfidf = classifier.named_steps['tfidf'].transform(text_data)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    return torch.tensor(X_tfidf.toarray(), dtype=torch.float32).to(device)\n",
    "\n",
    "# Update the PyTorch training call\n",
    "model, label_encoder = train_pytorch_model(classifier, X, y, n_epochs=22)\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# 1. First train the TF-IDF vectorizer\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# 2. Train PyTorch model\n",
    "model, label_encoder = train_pytorch_model(classifier, X, y, n_epochs=22)\n",
    "\n",
    "# 3. Make predictions\n",
    "def pytorch_predict(model, le, text_data):\n",
    "    with torch.no_grad():\n",
    "        X_tensor = prepare_tensor_features(text_data)\n",
    "        outputs = model(X_tensor)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "    return le.inverse_transform(predictions.cpu().numpy())\n",
    "\n",
    "pred_data['predicted_SSA'] = pytorch_predict(model, label_encoder, X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the data for prediction\n",
    "def predict_ssa(text_input, model, classifier, label_encoder):\n",
    "    # Prepare the text input\n",
    "    if isinstance(text_input, str):\n",
    "        text_input = pd.read_csv('../docs/nos-ofqual/NOS_Data_w_SSA_Industry.csv')\n",
    "    \n",
    "    # Preprocess the text using the same pipeline steps\n",
    "    processed_text = prepare_text_features(text_input)\n",
    "    \n",
    "    # Convert to tensor features\n",
    "    X_tensor = prepare_tensor_features(processed_text)\n",
    "    \n",
    "    # Get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "    # Convert predictions back to labels\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted.cpu().numpy())\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "# Example usage:\n",
    "# sample_text = \"Sample NOS text to classify\"\n",
    "# predictions = predict_ssa(sample_text, model, classifier, label_encoder)\n",
    "# print(f\"Predicted SSA: {predictions[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
