{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NOS_Data_w_SSA_Industry.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nos_ssa \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNOS_Data_w_SSA_Industry.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m nos_ssa\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m~/zavmo-api/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zavmo-api/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/zavmo-api/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zavmo-api/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/zavmo-api/.venv/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NOS_Data_w_SSA_Industry.csv'"
     ]
    }
   ],
   "source": [
    "nos_ssa = pd.read_csv(\"NOS_Data_w_SSA_Industry.csv\")\n",
    "nos_ssa.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14.1 Foundations for learning and life'\n",
      " '10.2 Archaeology and archaeological sciences'\n",
      " '7.1 Retailing and wholesaling' '1.4 Public services' nan\n",
      " '5.2 Building and construction' '15.3 Business management'\n",
      " '11.2 Sociology and social policy' '15.1 Accounting and finance'\n",
      " '4.1 Engineering' '3.1 Environmental conservation'\n",
      " '4.2 Manufacturing technologies' '9.2 Crafts, creative arts and design'\n",
      " '9.1 Performing arts' '3.4 Environmental conservation'\n",
      " '7.2 Warehousing and distribution' '7.4 Hospitality and catering'\n",
      " '3.3 Animal care and veterinary science' '3.2 Horticulture and forestry'\n",
      " '12.2 Other languages literature and culture' '15.4 Marketing and Sales'\n",
      " '1.3 Health and social care' '9.3 Media and communication'\n",
      " '13.1 Teaching and Lecturing' '2.1 Science'\n",
      " '4.3 Transportation operations and maintenance'\n",
      " '5.3 Urban, rural and regional planning'\n",
      " '6.1 Digital technology (practitioners)'\n",
      " '18.1 Publishing and information services'\n",
      " '8.1 Sport leisure and recreation' '3.1 Agriculture'\n",
      " '15.3 Business Management' '1.5 Child development and well-being'\n",
      " '13.2 Direct learning support'\n",
      " '1.2 Nursing, and subjects and vocations allied to medicine'\n",
      " '15.5 Law and legal services' '7.3 Service enterprises' '10.1 History']\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "SSA_unique_nos = nos_ssa['OFQUAL_SSA'].unique()\n",
    "print(SSA_unique_nos)\n",
    "print(len(SSA_unique_nos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9205\n",
      "16052\n"
     ]
    }
   ],
   "source": [
    "len_nos = len(nos_ssa)\n",
    "ssa_nos_count = len(nos_ssa['OFQUAL_SSA'].dropna())\n",
    "print(ssa_nos_count)\n",
    "print(len_nos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.34487914278594 % of NOS have SSA\n"
     ]
    }
   ],
   "source": [
    "percentage_nos = ((ssa_nos_count/len_nos) * 100) \n",
    "print(percentage_nos,\"% of NOS have SSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         nos_id                     NOS_Industry  \\\n",
      "0        CFAUE3         Understanding Enterprise   \n",
      "1      CCSAPAA2          Archaeological Practice   \n",
      "2        CFAUE6         Understanding Enterprise   \n",
      "3       IMIHR15                   Vehicle Rental   \n",
      "4        CFAUE7         Understanding Enterprise   \n",
      "...         ...                              ...   \n",
      "16044     SFHM3  Breast Screening and Assessment   \n",
      "16045     SFHM9  Breast Screening and Assessment   \n",
      "16048      REC2                      Recruitment   \n",
      "16049    CFAUE5         Understanding Enterprise   \n",
      "16050      REC1                      Recruitment   \n",
      "\n",
      "                                                   title  \\\n",
      "0            Knowing the market and satisfying customers   \n",
      "1                                    Commission Research   \n",
      "2                                   Planning for Success   \n",
      "3      Receive goods, equipment or vehicles for hire ...   \n",
      "4                                 Managing Money Matters   \n",
      "...                                                  ...   \n",
      "16044  Review mammography images for the presence of ...   \n",
      "16045     Undertake a clinical examination of the breast   \n",
      "16048  Contact hirers and establish recruitment requi...   \n",
      "16049                           Winning Help and Support   \n",
      "16050                      Develop a resourcing strategy   \n",
      "\n",
      "                                                    text  \\\n",
      "0      **NOS ID:** CFAUE3  \\n**Title:** Knowing the m...   \n",
      "1      **NOS ID:** CCSAPAA2  \\n**Title:** Commission ...   \n",
      "2      **NOS ID:** CFAUE6  \\n**Title:** Planning for ...   \n",
      "3      **NOS ID:** IMIHR15  \\n**Title:** Receive good...   \n",
      "4      **NOS ID:** CFAUE7  \\n**Title:** Managing Mone...   \n",
      "...                                                  ...   \n",
      "16044  **NOS ID:** sfhm3  \\n**Title:** Review mammogr...   \n",
      "16045  **NOS ID:** sfhm9  \\n**Title:** Undertake a cl...   \n",
      "16048  **NOS ID:** REC2  \\n**Title:** Contact hirers ...   \n",
      "16049  **NOS ID:** CFAUE5  \\n**Title:** Winning Help ...   \n",
      "16050  **NOS ID:** REC1  \\n**Title:** Develop a resou...   \n",
      "\n",
      "                                              OFQUAL_SSA  \n",
      "0                 14.1 Foundations for learning and life  \n",
      "1           10.2 Archaeology and archaeological sciences  \n",
      "2                 14.1 Foundations for learning and life  \n",
      "3                          7.1 Retailing and wholesaling  \n",
      "4                 14.1 Foundations for learning and life  \n",
      "...                                                  ...  \n",
      "16044  1.2 Nursing, and subjects and vocations allied...  \n",
      "16045  1.2 Nursing, and subjects and vocations allied...  \n",
      "16048                           15.3 Business management  \n",
      "16049             14.1 Foundations for learning and life  \n",
      "16050                           15.3 Business management  \n",
      "\n",
      "[9196 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "nos_ssa_list = nos_ssa[['nos_id', 'NOS_Industry', 'title', 'text', 'OFQUAL_SSA']].dropna()\n",
    "print(nos_ssa_list)\n",
    "nos_ssa_list.to_csv('nos_ssa_list_57%.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. Data Preparation\n",
    "def prepare_text_features(nos_data):\n",
    "    # Fill NaN values with empty strings\n",
    "    title = nos_data['title'].fillna('')\n",
    "    text = nos_data['text'].fillna('')\n",
    "    industry = nos_data['NOS_Industry'].fillna('')\n",
    "    \n",
    "    # Combine relevant text fields\n",
    "    return title + ' ' + text + ' ' + industry\n",
    "\n",
    "# 2. Create the classification pipeline\n",
    "def create_classifier():\n",
    "    return Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english'\n",
    "        )),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced'\n",
    "        )))\n",
    "    ])\n",
    "\n",
    "# 3. Training and Evaluation\n",
    "def train_and_evaluate(X, y):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    classifier = create_classifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = classifier.score(X_train, y_train)\n",
    "    test_score = classifier.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Training accuracy: {train_score:.3f}\")\n",
    "    print(f\"Testing accuracy: {test_score:.3f}\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "# 4. Prediction with confidence scores\n",
    "def predict_with_confidence(classifier, text):\n",
    "    # Handle NaN values in prediction data\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = text.fillna('')\n",
    "    \n",
    "    # Get probability scores\n",
    "    proba = classifier.predict_proba(text)\n",
    "    predictions = classifier.predict(text)\n",
    "    \n",
    "    # Get confidence scores for each prediction\n",
    "    confidence_scores = np.max(proba, axis=1)\n",
    "    \n",
    "    return predictions, confidence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nos_ssa_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load training data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mnos_ssa_list\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Prepare features and labels\u001b[39;00m\n\u001b[1;32m      8\u001b[0m X \u001b[38;5;241m=\u001b[39m prepare_text_features(nos_ssa_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nos_ssa_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Load and prepare training data\n",
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "nos_ssa_list\n",
    "\n",
    "# Prepare features and labels\n",
    "X = prepare_text_features(nos_ssa_list)\n",
    "y = nos_ssa_list['OFQUAL_SSA']\n",
    "\n",
    "# Train the model\n",
    "classifier = train_and_evaluate(X, y)\n",
    "\n",
    "# Load prediction data\n",
    "pred_data = pd.read_csv('NOS_Data_w_SSA_Industry.csv')\n",
    "\n",
    "# Prepare prediction features\n",
    "X_pred = prepare_text_features(pred_data)\n",
    "\n",
    "# Make predictions with confidence scores\n",
    "predictions, confidence_scores = predict_with_confidence(classifier, X_pred)\n",
    "\n",
    "# Add predictions and confidence scores to prediction dataframe\n",
    "pred_data['predicted_SSA'] = predictions\n",
    "pred_data['confidence_score'] = confidence_scores\n",
    "\n",
    "# Display sample of predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "print(pred_data[['nos_id', 'NOS_Industry', 'OFQUAL_SSA', 'predicted_SSA', 'confidence_score']].head())\n",
    "\n",
    "# Save predictions\n",
    "pred_data.to_csv('nos_ssa_predictions.csv', index=False)\n",
    "print(\"\\nPredictions saved to nos_ssa_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classifier\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m classifier \u001b[38;5;241m=\u001b[39m train_and_evaluate_multiple_epochs(classifier, \u001b[43mX\u001b[49m, y, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m    106\u001b[0m X_pred \u001b[38;5;241m=\u001b[39m prepare_text_features(pred_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Create classifier with SGD\n",
    "# Modified feature engineering\n",
    "classifier = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=150000,  # 3x increase from 50k\n",
    "        ngram_range=(1, 4),   # Carefully expanded\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True,\n",
    "        min_df=2,            # Slightly more permissive\n",
    "        max_df=0.6,          # Slightly more restrictive\n",
    "        analyzer='word',     # New - explicit analyzer\n",
    "        norm='l2'            # New - better normalization\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(SGDClassifier(\n",
    "        loss='log_loss',\n",
    "        penalty='elasticnet',\n",
    "        alpha=1e-5,          # Reduced regularization\n",
    "        learning_rate='adaptive',\n",
    "        eta0=0.2,           # Higher initial rate\n",
    "        power_t=0.25,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.25,  # More validation data\n",
    "        n_iter_no_change=7, # More patience\n",
    "        random_state=42,\n",
    "        class_weight='balanced'  # New - handle class imbalance\n",
    "    )))\n",
    "])\n",
    "\n",
    "\n",
    "def train_and_evaluate_multiple_epochs(classifier, X, y, n_epochs=5, n_splits=5):\n",
    "    # Initialize KFold cross-validator\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Track metrics across epochs\n",
    "    epoch_metrics = []\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    # Split into train/test once to have consistent test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Initial fit to transform the data\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Adjust learning rate dynamically\n",
    "        new_eta = 0.1 / (1 + 0.2 * epoch)  # Decaying learning rate\n",
    "        for estimator in classifier.named_steps['clf'].estimators_:\n",
    "            estimator.eta0 = new_eta\n",
    "            \n",
    "        # Partial fit for incremental learning\n",
    "        classifier.named_steps['clf'].partial_fit(\n",
    "            classifier.named_steps['tfidf'].transform(X_train),\n",
    "            y_train,\n",
    "            classes=np.unique(y)\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        train_score = classifier.score(X_train, y_train)\n",
    "        test_score = classifier.score(X_test, y_test)\n",
    "        \n",
    "        # Cross validation score\n",
    "        cv_scores = cross_val_score(classifier, X_train, y_train, cv=kf)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}:\")\n",
    "        print(f\"Training accuracy: {train_score:.3f}\")\n",
    "        print(f\"Testing accuracy: {test_score:.3f}\")\n",
    "        print(f\"Cross-validation mean: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_metrics.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_score': train_score,\n",
    "            'test_score': test_score,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std()\n",
    "        })\n",
    "        \n",
    "        # Update best state if improved\n",
    "        if test_score > best_accuracy:\n",
    "            best_accuracy = test_score\n",
    "            best_state = {\n",
    "                'tfidf': classifier.named_steps['tfidf'],\n",
    "                'clf': classifier.named_steps['clf']\n",
    "            }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nTraining Summary:\")\n",
    "    for metric in epoch_metrics:\n",
    "        print(f\"Epoch {metric['epoch']}: \"\n",
    "              f\"Train={metric['train_score']:.3f}, \"\n",
    "              f\"Test={metric['test_score']:.3f}, \"\n",
    "              f\"CV={metric['cv_mean']:.3f}Â±{metric['cv_std']*2:.3f}\")\n",
    "    \n",
    "    # Restore best state\n",
    "    classifier.named_steps['tfidf'] = best_state['tfidf']\n",
    "    classifier.named_steps['clf'] = best_state['clf']\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "# Train the classifier\n",
    "classifier = train_and_evaluate_multiple_epochs(classifier, X, y, n_epochs=20)\n",
    "\n",
    "# Make predictions\n",
    "X_pred = prepare_text_features(pred_data)\n",
    "predictions, confidence_scores = predict_with_confidence(classifier, X_pred)\n",
    "\n",
    "# Add predictions and confidence scores\n",
    "pred_data['predicted_SSA'] = predictions\n",
    "pred_data['confidence_score'] = confidence_scores\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\nSample predictions with best model:\")\n",
    "sample_df = pred_data[['nos_id', 'NOS_Industry', 'OFQUAL_SSA', 'predicted_SSA', 'confidence_score']].head()\n",
    "print(sample_df)\n",
    "\n",
    "# Save predictions\n",
    "pred_data.to_csv('nos_ssa_predictions.csv', index=False)\n",
    "print(\"\\nPredictions saved to nos_ssa_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43;01mPrepareData\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Modified data preparation functions\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mprepare_text_features\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnos_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250;43m        \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Handle both DataFrames and preprocessed Series\"\"\"\u001b[39;49;00m\n",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m, in \u001b[0;36mPrepareData\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(X_tfidf\u001b[38;5;241m.\u001b[39mtoarray(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Update the PyTorch training call\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m model, label_encoder \u001b[38;5;241m=\u001b[39m train_pytorch_model(classifier, \u001b[43mX\u001b[49m, y, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check for MPS availability\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class PrepareData():\n",
    "    # Modified data preparation functions\n",
    "    def prepare_text_features(nos_data):\n",
    "        \"\"\"Handle both DataFrames and preprocessed Series\"\"\"\n",
    "        if isinstance(nos_data, pd.DataFrame):\n",
    "            # For raw DataFrames\n",
    "            return nos_data['title'].fillna('') + ' ' + \\\n",
    "                nos_data['text'].fillna('') + ' ' + \\\n",
    "                nos_data['NOS_Industry'].fillna('')\n",
    "        # For already processed text\n",
    "        return nos_data.fillna('')\n",
    "\n",
    "    def prepare_tensor_features(text_data):\n",
    "        \"\"\"Handle preprocessed text directly\"\"\"\n",
    "        # Vectorize using existing TF-IDF\n",
    "        X_tfidf = classifier.named_steps['tfidf'].transform(text_data)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        return torch.tensor(X_tfidf.toarray(), dtype=torch.float32).to(device)\n",
    "\n",
    "    # Update the PyTorch training call\n",
    "    model, label_encoder = train_pytorch_model(classifier, X, y, n_epochs=50)\n",
    "\n",
    "# 2. Create PyTorch model\n",
    "class SSAClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_size, num_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# 3. Modified training function\n",
    "def train_pytorch_model(classifier, X, y, n_epochs=50):\n",
    "    # Convert labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_tensor = torch.tensor(y_encoded, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get tensor features\n",
    "    X_tensor = prepare_tensor_features(X)\n",
    "    \n",
    "    # Create model\n",
    "    input_size = X_tensor.shape[1]\n",
    "    num_classes = len(le.classes_)\n",
    "    model = SSAClassifier(input_size, num_classes).to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_x, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            acc = (predicted == y_tensor).float().mean()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss/len(loader):.4f}, Acc={acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    return model, le\n",
    "\n",
    "# Usage:\n",
    "# 1. First train the TF-IDF vectorizer\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# 2. Train PyTorch model\n",
    "model, label_encoder = train_pytorch_model(classifier, X, y, n_epochs=22)\n",
    "\n",
    "# 3. Make predictions\n",
    "def pytorch_predict(model, le, text_data):\n",
    "    with torch.no_grad():\n",
    "        X_tensor = prepare_tensor_features(text_data)\n",
    "        outputs = model(X_tensor)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "    return le.inverse_transform(predictions.cpu().numpy())\n",
    "\n",
    "pred_data['predicted_SSA'] = pytorch_predict(model, label_encoder, X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(X_tfidf\u001b[38;5;241m.\u001b[39mtoarray(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# 4. Now we can call the training\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m model, label_encoder \u001b[38;5;241m=\u001b[39m train_pytorch_model(classifier, \u001b[43mX\u001b[49m, y, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=150000,  # 3x increase from 50k\n",
    "        ngram_range=(1, 4),   # Carefully expanded\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True,\n",
    "        min_df=2,            # Slightly more permissive\n",
    "        max_df=0.6,          # Slightly more restrictive\n",
    "        analyzer='word',     # New - explicit analyzer\n",
    "        norm='l2'            # New - better normalization\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(SGDClassifier(\n",
    "        loss='log_loss',\n",
    "        penalty='elasticnet',\n",
    "        alpha=1e-5,          # Reduced regularization\n",
    "        learning_rate='adaptive',\n",
    "        eta0=0.2,           # Higher initial rate\n",
    "        power_t=0.25,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.25,  # More validation data\n",
    "        n_iter_no_change=7, # More patience\n",
    "        random_state=42,\n",
    "        class_weight='balanced'  # New - handle class imbalance\n",
    "    )))\n",
    "])\n",
    "\n",
    "\n",
    "# Check for MPS availability\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Define PyTorch model first\n",
    "class SSAClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_size, num_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# 2. Define training function\n",
    "def train_pytorch_model(classifier, X, y, n_epochs=22):\n",
    "    # Convert labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_tensor = torch.tensor(y_encoded, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get tensor features\n",
    "    X_tensor = prepare_tensor_features(X)\n",
    "    \n",
    "    # Create model\n",
    "    input_size = X_tensor.shape[1]\n",
    "    num_classes = len(le.classes_)\n",
    "    model = SSAClassifier(input_size, num_classes).to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    best_acc = 0\n",
    "    patience = 5  # Early stopping patience\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_x, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            acc = (predicted == y_tensor).float().mean()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss/len(loader):.4f}, Acc={acc:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    return model, le\n",
    "\n",
    "# 3. Define data preparation functions\n",
    "def prepare_text_features(nos_data):\n",
    "    \"\"\"Handle both DataFrames and preprocessed Series\"\"\"\n",
    "    if isinstance(nos_data, pd.DataFrame):\n",
    "        return nos_data['title'].fillna('') + ' ' + \\\n",
    "               nos_data['text'].fillna('') + ' ' + \\\n",
    "               nos_data['NOS_Industry'].fillna('')\n",
    "    return nos_data.fillna('')\n",
    "\n",
    "def prepare_tensor_features(text_data):\n",
    "    \"\"\"Handle preprocessed text directly\"\"\"\n",
    "    X_tfidf = classifier.named_steps['tfidf'].transform(text_data)\n",
    "    return torch.tensor(X_tfidf.toarray(), dtype=torch.float32).to(device)\n",
    "\n",
    "# 4. Now we can call the training\n",
    "model, label_encoder = train_pytorch_model(classifier, X, y, n_epochs=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
